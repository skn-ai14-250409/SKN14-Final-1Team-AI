{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 9/9 기준"
      ],
      "metadata": {
        "id": "DOKCpqCwqqcf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-openai langchain-huggingface chromadb gdown"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "VOPvTjtuC0Fd",
        "outputId": "7d7c7430-bfe4-48a8-8436-49b973daf7a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.8/510.8 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m93.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.2/452.2 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "EII-voX7DIPT",
        "outputId": "4bd34b86-2a4f-4673-df7c-5bd5e4c2fee6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.29-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=0.3.75 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.3.75)\n",
            "Requirement already satisfied: langchain<2.0.0,>=0.3.27 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.3.27)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.43)\n",
            "Collecting requests<3,>=2.32.5 (from langchain-community)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.6.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.10.1)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.23)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.6.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.6.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain<2.0.0,>=0.3.27->langchain-community) (0.3.11)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain<2.0.0,>=0.3.27->langchain-community) (2.11.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.75->langchain-community) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.75->langchain-community) (4.15.0)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.75->langchain-community) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community) (0.24.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain-community) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain-community) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<2.0.0,>=0.3.75->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.27->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.27->langchain-community) (2.33.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.6.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.3.1)\n",
            "Downloading langchain_community-0.3.29-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: requests, mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain-community\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.4\n",
            "    Uninstalling requests-2.32.4:\n",
            "      Successfully uninstalled requests-2.32.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dataclasses-json-0.6.7 langchain-community-0.3.29 marshmallow-3.26.1 mypy-extensions-1.1.0 requests-2.32.5 typing-inspect-0.9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "requests"
                ]
              },
              "id": "1b1f25b248f64bdfbae4325215d60dc3"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langgraph -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2h3XcviqDWBD",
        "outputId": "86189849-6c95-438a-f23d-c4657c7eec26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.3/153.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.7/216.7 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQ0BEhIECW8Z"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict, List, Dict, Any, Optional\n",
        "from pathlib import Path\n",
        "import os\n",
        "import shutil\n",
        "import tempfile\n",
        "import gdown\n",
        "import json\n",
        "import torch\n",
        "\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langgraph.checkpoint.memory import MemorySaver"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 벡터DB 다운\n",
        "- vector_db.py\n",
        "- -> 코랩 기준으로 파일 저장 위치 수정"
      ],
      "metadata": {
        "id": "W9XVwSYbCj_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_drive_folder_to_chroma_db(folder_url: str, target_dir: Path):\n",
        "    target_dir = Path(target_dir).resolve()\n",
        "    target_dir.mkdir(parents=True, exist_ok=True)\n",
        "    with tempfile.TemporaryDirectory() as td:\n",
        "        gdown.download_folder(url=folder_url, output=td, quiet=False, use_cookies=False)\n",
        "        entries = [Path(td) / name for name in os.listdir(td)]\n",
        "        src_root = entries[0] if len(entries) == 1 and entries[0].is_dir() else Path(td)\n",
        "        for p in src_root.iterdir():\n",
        "            dst = target_dir / p.name\n",
        "            if dst.exists():\n",
        "                shutil.rmtree(dst) if dst.is_dir() else dst.unlink()\n",
        "            shutil.move(str(p), str(dst))\n",
        "    if not (target_dir / \"chroma.sqlite3\").exists():\n",
        "        raise RuntimeError(f\"'chroma.sqlite3'가 없습니다: {target_dir}\")\n",
        "\n",
        "def create_chroma_db():\n",
        "    HERE = Path(\".\").resolve()\n",
        "    FOLDER_URL = \"https://drive.google.com/drive/folders/1_paLIqOIeOyozE-wsKuMO4wIiOpaLio9\"\n",
        "    download_drive_folder_to_chroma_db(FOLDER_URL, HERE / \"chroma_db\")"
      ],
      "metadata": {
        "id": "QOenS6h4CeX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## retriever.py\n",
        "- -> 코랩 기준으로 파일 저장 위치 수정"
      ],
      "metadata": {
        "id": "37Q5lZsbC4be"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HERE = \".\"\n",
        "DB_DIR = os.path.join(HERE, \"chroma_db\")\n",
        "COLLECTION_NAME = \"qna_collection\"\n",
        "EMBED_MODEL = \"BAAI/bge-m3\"\n",
        "TOP_K = 5\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=EMBED_MODEL,\n",
        "    encode_kwargs={\"normalize_embeddings\": True},\n",
        ")\n",
        "\n",
        "def retriever_setting():\n",
        "    if not os.path.isdir(DB_DIR):\n",
        "        print(\"ChromaDB 디렉토리가 없습니다. 구글 드라이브에서 다운로드합니다.\")\n",
        "        create_chroma_db()\n",
        "    vs = Chroma(\n",
        "        collection_name=COLLECTION_NAME,\n",
        "        persist_directory=DB_DIR,\n",
        "        embedding_function=embeddings,\n",
        "    )\n",
        "    retriever = vs.as_retriever(search_kwargs={\"k\": TOP_K})\n",
        "    return retriever"
      ],
      "metadata": {
        "id": "ZO0oPI6YC6Yw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## rag2.py & rag.py\n",
        "- basic_chain_setting... 어떤 파일 함수를 써야 하는지?"
      ],
      "metadata": {
        "id": "8PvRU038DgRz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def basic_chain_setting(): # rag2.py\n",
        "    llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "\n",
        "    basic_prompt = PromptTemplate.from_template(\n",
        "        \"\"\"\n",
        "    당신은 api 문서 관련 전문 챗봇으로서 사용자의 질문에 정확하고 친절하게 답변해야 합니다.\n",
        "    아래 제공되는 문서에 없는 내용은 절대 답변에 포함하지 말고, 문서 내에서만 답변 내용을 찾아서 제공하세요.\n",
        "    만약 사용자 질문이 구글 api 문서에 대한 질문이 아니라면, 아래 문서는 무시하고 일상 질문에 대해서만 답변하세요.\n",
        "\n",
        "    문서 : {context}\n",
        "\n",
        "    이전 대화 내역 : {history}\n",
        "\n",
        "    이번 사용자 질문 : {question}\n",
        "    \"\"\"\n",
        "    )\n",
        "\n",
        "    basic_chain = basic_prompt | llm | StrOutputParser()\n",
        "\n",
        "    return basic_chain\n",
        "\n",
        "\n",
        "def query_setting():\n",
        "    llm = ChatOpenAI(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        temperature=0,\n",
        "        model_kwargs={\"response_format\": {\"type\": \"json_object\"}},\n",
        "    )\n",
        "\n",
        "    query_prompt = PromptTemplate.from_template(\n",
        "        \"\"\"\n",
        "       유저의 채팅 히스토리와 현재 질문이 주어집니다.\n",
        "\n",
        "\n",
        "       **중요**: 이전 대화 맥락을 반드시 고려해서 질문을 생성하세요.\n",
        "       - 현재 질문이 이전 대화와 연관되어 있다면, 이전 맥락을 포함한 통합된 질문을 만들어주세요.\n",
        "\n",
        "       - 예: 바로 전에 \"People API 연락처 조회\"에 대해 이야기하고 나서, \"그럼 프로필 수정은?\"이라는 질문이 나오면 \"People API에서 프로필 수정 방법\"으로 통합해주세요.\n",
        "       - 주의사항: 이전에 \"People API 연락첯 조회\"에 대해 이야기하고 나서, \"Firebase\"와 같이 다른 api에 대한 대화 내용이 나온 후 \"프로필 수정은?\"이라는 질문이 나오면 마지막 대화 맥락에 맞춰서, \"Firebase에서 프로필 수정 방법\"과 같이 통합해야 합니다.\n",
        "\n",
        "       - 이전 대화에서 이미 답변이 나온 질문은 생성하지 마세요.\n",
        "       - 질문은 1개가 될 수도 있고 여러개가 될 수도 있습니다.\n",
        "\n",
        "       대화 히스토리: {rewritten}\n",
        "\n",
        "       JSON 반환 형태:\n",
        "       {{\"questions\": [\"맥락을 고려한 통합 질문 1\", \"맥락을 고려한 통합 질문 2\", ...]}}\n",
        "       \"\"\"\n",
        "    )\n",
        "\n",
        "    def parse_json(response):\n",
        "        return json.loads(response.content)  # response.content 사용\n",
        "\n",
        "    chain = query_prompt | llm | parse_json\n",
        "    return chain"
      ],
      "metadata": {
        "id": "HgZ0d_WoDhpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## langgraph_node.py"
      ],
      "metadata": {
        "id": "sd4MZmUgDxJT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_F1\")"
      ],
      "metadata": {
        "id": "sLYWDthMEUiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "basic_chain = basic_chain_setting()\n",
        "retriever = retriever_setting()\n",
        "query_chain = query_setting()\n",
        "\n",
        "\n",
        "class ChatState(TypedDict, total=False):\n",
        "    question: str  # 유저 질문\n",
        "    answer: str  # 모델 답변\n",
        "    rewritten: str  # 통합된 질문\n",
        "    queries: List[str]  # 쿼리(질문들)\n",
        "    search_results: List[str]  # 벡터 DB 검색 결과들\n",
        "    messages: List[Dict[str, str]]  # 사용자 및 모델의 대화 히스토리\n",
        "\n",
        "\n",
        "# (1) 사용자 질문 + 히스토리 통합 → 통합된 질문과 쿼리 추출\n",
        "def extract_queries(state: ChatState) -> ChatState:\n",
        "    user_text = state[\"question\"]\n",
        "\n",
        "    # 히스토리에서 최근 몇 개의 메시지를 가져와서 통합 질문을 생성\n",
        "    messages = state.get(\"messages\", [])\n",
        "\n",
        "    # 최근 4개 메시지만 사용\n",
        "    history_tail = messages[-4:] if messages else []\n",
        "    context = history_tail.copy()\n",
        "\n",
        "    # 현재 사용자 질문 추가\n",
        "    context.append({\"role\": \"user\", \"content\": user_text})\n",
        "    state[\"rewritten\"] = context\n",
        "\n",
        "    return state\n",
        "\n",
        "\n",
        "# (2) LLM에게 질문 분리를 시킨다\n",
        "def split_queries(state: ChatState) -> ChatState:\n",
        "    rewritten = state.get(\"rewritten\")\n",
        "\n",
        "    response = query_chain.invoke({\"rewritten\": rewritten})\n",
        "    state[\"queries\"] = response[\"questions\"]  # questions 리스트만 저장\n",
        "\n",
        "    return state\n",
        "\n",
        "\n",
        "# (3) 벡터 DB 툴 호출\n",
        "def search_tool(query: str):\n",
        "    \"\"\"질문을 바탕으로 벡터 DB에서 결과 검색\"\"\"\n",
        "    return retriever.invoke(query)  # retriever는 DB 검색 로직을 호출\n",
        "\n",
        "\n",
        "# (4) 기본 답변 생성 노드\n",
        "def langgraph_node(state: ChatState) -> Dict[str, Any]:\n",
        "    history = state.get(\"messages\", [])\n",
        "    \"\"\"질문에 대한 기본 답변 생성\"\"\"\n",
        "    queries = state[\"queries\"]\n",
        "    print(f\"생성된 질문 리스트 {queries}\")\n",
        "    search_results = []\n",
        "\n",
        "    # 각 쿼리마다 벡터 DB 검색\n",
        "    for query in queries:\n",
        "        print(f\"{query} 검색중...\")\n",
        "        results = search_tool(query)\n",
        "        search_results.append(results)  # 검색된 결과들을 모아서 저장\n",
        "\n",
        "    # 검색된 결과를 바탕으로 답변 생성\n",
        "    answer = basic_chain.invoke(\n",
        "        {\n",
        "            \"question\": state[\"question\"],\n",
        "            \"history\": history,\n",
        "            \"context\": \"\\n\".join([str(res) for res in search_results]),\n",
        "        }\n",
        "    ).strip()\n",
        "\n",
        "    state[\"search_results\"] = search_results\n",
        "    state[\"answer\"] = answer\n",
        "\n",
        "    return state  # 답변을 반환"
      ],
      "metadata": {
        "id": "3QWOhqF8Dx6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## langgraph_setting_edit.py\n",
        "- langgraph_setting.py 이건 쓰이는지?"
      ],
      "metadata": {
        "id": "Gm8B68XID5jT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 그래프 설정\n",
        "def graph_setting_edit():\n",
        "    # LangGraph 정의\n",
        "    graph = StateGraph(ChatState)\n",
        "\n",
        "    # 노드 등록\n",
        "    graph.add_node(\"extract_queries\", extract_queries)  # 질문 통합 + 쿼리 추출 노드\n",
        "    graph.add_node(\"split_queries\", split_queries)  # 질문 분리 툴\n",
        "    graph.add_node(\"basic\", langgraph_node)  # 기본 답변 노드\n",
        "\n",
        "    # 시작 노드 정의\n",
        "    graph.set_entry_point(\"extract_queries\")\n",
        "\n",
        "    # 흐름 설정\n",
        "    graph.add_edge(\"extract_queries\", \"split_queries\")  # 질문 추출 후 분리\n",
        "    graph.add_edge(\"split_queries\", \"basic\")  # 쿼리 분리 후 기본 답변 노드로 넘어감\n",
        "    graph.add_edge(\"basic\", END)  # 기본 답변 후 종료\n",
        "\n",
        "    # 그래프 컴파일\n",
        "    memory = MemorySaver()\n",
        "    compiled_graph = graph.compile(checkpointer=memory)\n",
        "\n",
        "    return compiled_graph"
      ],
      "metadata": {
        "id": "bVojgqWPD6Oz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 실행\n",
        "- main2.py"
      ],
      "metadata": {
        "id": "_1ZjLRGYD8K8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "graph = graph_setting_edit()\n",
        "\n",
        "\n",
        "def run_langraph(user_input, config_id, chat_history=None):\n",
        "    config = {\"configurable\": {\"thread_id\": config_id}}\n",
        "\n",
        "    # chat_history가 None이면 빈 리스트로 초기화\n",
        "    if chat_history is None:\n",
        "        chat_history = []\n",
        "\n",
        "    result = graph.invoke(\n",
        "        {\"messages\": chat_history, \"question\": user_input}, config=config\n",
        "    )\n",
        "\n",
        "    return result[\"answer\"]"
      ],
      "metadata": {
        "id": "yHYtTKLSD_zj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## test"
      ],
      "metadata": {
        "id": "Y_wwL3DREc7K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_history = [\n",
        "    {\"role\": \"user\", \"content\": \"firebase auth의 인증 설정에서 응답 형식은 어떻게 되나요?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"firebase의 인증 설정 응답은 JSON 형식으로 반환됩니다.\"},\n",
        "    {\"role\": \"user\", \"content\": \"people api의 프로필 설정 방식에서 응답 형식은 어떻게 되나요?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"people api의 프로필 설정 응답은 JSON 형식으로 반환됩니다.\"}\n",
        "]"
      ],
      "metadata": {
        "id": "My-OczMEEeXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = run_langraph('내가 방금 뭐 물어봤어?', 'test1', dummy_history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwM_lu1TEh8y",
        "outputId": "932d296d-a660-4e17-b009-4a2f8c085a26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "생성된 질문 리스트 ['People API의 프로필 설정 응답 형식은 어떻게 되나요?']\n",
            "People API의 프로필 설정 응답 형식은 어떻게 되나요? 검색중...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oc6yN7R7EiSq",
        "outputId": "d8ae2f7e-3e71-4565-8cfb-2ff129ecd42c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "사용자님께서는 방금 \"people api의 프로필 설정 방식에서 응답 형식은 어떻게 되나요?\"라고 질문하셨습니다.\n"
          ]
        }
      ]
    }
  ]
}